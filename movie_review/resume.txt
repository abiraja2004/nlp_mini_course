Rizwan Mian, PhD	Big Data Scientist & ConsultantLinkedIn: http://ca.linkedin.com/in/rizwanmian	647 533 4249, skype: vizvanToronto, Canada.	vizvan@gmail.com		Citizenship: Canadian	1-line profileex-Hortonworks in Manhattan. PhD Queen’s, postdoc McGill, York. Seasoned Data Scientist and Hadoop Architect. Humanist.SummaryData Science:	Hands-on experience in complete data science lifecycle including data gathering, preparation, transformation, mining, validation and presentation.Big Data & Clouds:	Expert in Big Data, cloud computing and batch processing.Customer Facing:	Experienced in customer facing and consulting roles including Telecom, Financial and Retail. Leadership:	Capable of exploring uncharted territory, including leading and developing new initiatives. Thrive in conducting Proof-of-Concept (PoC) exercises.Analytical:	Enjoy performance and numerical analysis of applications and systems, building budgeting and performance models and generating forecasts.Fluid & Agile:	Able to transform into multiple roles efficiently and effectively to get the job done.Problem Solver:	Thinking 'out-of-box' and applying most appropriate methods to solve problems.Communication:	Proven communication skills in offering consulting services to multiple customers, presenting at international conferences, and working in international groups. Holistic Engineer:	Hands-on experience in complete software development lifecycle (SDLC) ranging from gathering customer requirements, development, testing and documentation.Computing Skills* Big Data/Hadoop:	Hortonworks/Cloudera (5y), HDInsight, Hive/Pig (3y), NoSQL (1y), Sqoop/Flume (1y)* Machine Learning:	Python Scikit-learn (1y), RapidMiner/Weka (1.5y), AzureML, Radoop (6m)* Statistical Tools:	Microsoft Excel (4y), SPSS/R (6m), MATLAB (6m) * Clouds, Grids:	Amazon/Open Stack Cloud (4.5y), Symphony Grid (3y), LSF (1y)* Databases/Webservers:	MySQL (3y), PostgreSQL (6m), Apache Tomcat (4y), phpMyAdmin (1y)* Programming:	Java (5.5y), C/C++ (3.5y), Perl (2y), Github (3y), CVS (8.5y)* Editors:	IntelliJ (2y), Eclipse (4y), Visual Studio (1y)* Misc.:	Agile Development (7y), Linux (8y), Windows (15y), Mac OS (3y), Data Science and Enterprise Training & CertificationsSummer 2017, Machine Learning, Stanford University, CA, USA.Winter 2014, Technology Entrepreneurship, Stanford University, CA, USA.Hortonworks’ 2.x Courses and Certifications: Data Science, Analyst, Java Developer and AdministratorSummer 2015, Data Mining Using Weka, University of Waikato, New ZealandFall 2011, Artificial Intelligence, Stanford University, CA, USA.Public Data Science Projects and Activities* Next AI Finalist. A global innovation hub for AI value creation and commercialization. Teams have access to capital ($200,000), world-renowned scientists, and network of business leaders and entrepreneurs: https://goo.gl/9Ezfbc * Developed and co-published ML as a Service model for 407: https://goo.gl/7Fkzmn * Building ML models to predict whether Dragons would invest in a pitch (github): https://goo.gl/NkRH2b   * Developed http://budgetnow.ca/ -- an express household budgeting system at no cost, downloads or profile.* Data Science blogs: http://dataplusplus.ca/blog/ Bell, Greater Toronto Area (http://www.bell.ca/Mobility) 	        Mar 16 – to dateCanada's major teleco, providing a comprehensive and innovative suite of broadband communications and content services.Big Data Specialist (contract)* Working at many fronts with an increasingly deep focus on analytics* Filtering Engine (2 citations): Developing disruptive filtering engine to slash away false positives in application alerts. The status of alerts, without inspection, is unknown. When inspected, about 70% of the alerts need no attention (false positives), but burn valuable devops cycles -- 2 days in a week. Building an analytical model based on heuristics to adjust the value of thresholds for filtering. * KPI development (2c): Quantified the Hadoop eco system from generating ingestion and consumption KPIs. They summarize the functioning of data ingestion, and its usage by end users. For example, they include average response time of a report. These KPI are developed in-house and are not available “out-of-the-box”.* Big Data dashboard (1c): Wire framed and led implementation of Big Data dashboard for the team.* Baseline Queries (1c): To keep customer experience consistent, identified queries that simulate user interaction. These queries execute daily and the results are displayed in Big Data dashboard. * Application Baseline (4c): Established baselines for use case and/or the application. This allows us to quantify and standardize “performance” of applications. The baselines are used for comparison with new numbers when changing the application or adding new use-cases.* System Baseline (1c): Quantified baselines for the Hadoop ecosystem. This allows us to compare the performance impact of changes in the Hadoop cluster.* Upgrade (13c): Developed sanity validation tests for the applications to ensure no impact is caused by the cluster upgrade. Then, upgraded Cloudera Manager and CDH in dev, pre prod and prod. Co-led node migration from preprod to prod.* Load Balancer (6c): setting up and validation of F5 and haproxy load balancers in dev, preprod and prod with Kerberos.* Use Case development: Support devops in implementing use cases. Also, implement when required. * Peer-education (2c): new hire quick-start. co-mentored multiple new hires. Developed a co-learning environment using wiki.* Data Dictionary (3c): Annotated application data with “business” meta data e.g. peak call rate, to enrich with context for non-Hadoop specialists like managers and analysts. Work included MVP, show case, quick start and presentations.* Reporting: Keeping management informed with weekly 1-pagers. The report is structured and numeric.* Gitlab (1c): setup gitlab access for the entire team, quick start, showcase and team activity. * Support (3c): On rotational support. Switched with peers when needed.Major Development Tools: Cloudera, Hadoop, HDFS, Impala, Sqoop, Kerberos, MySQL, Python, Perl, Microsoft Excel, Intellij, Gitlab, Linux, Windows.HORTONWORKS, North/Central US and Greater Toronto Area (http://hortonworks.com/) 	        Jul 14 – Mar 16 (1y,9m)As a Big Data leader, Hortonworks is enabling a modern data architecture with enterprise Apache Hadoop.Systems Data Architect* Setup Linux HDInsight in Azure for an Industrial client. Ingested on-site MSSQL Server data into HDInsight. Architected and co-developed revenue generating application for the client. The application linked client data with publicly available permit data and identified under equipped machinery in the field that is a candidate for upgrade, and hence revenue.* Consultant Architect at a major Canadian Telco. Mentored Kerberization of development and production clusters. Facilitated productionizing of workloads with hands-on where needed. Enabled business team to query data in Hadoop cluster that provided exposure to higher management and showed business value of Hadoop. Authored implementation plans for Upgrade, setting up Authorization, and introducing Smart Monitoring. Knowledge transfer with support portal, online training, logging tickets and community forums.* Resident Architect for a major international bank in New York City. Built a centralized data governance platform using Hadoop to support multiple applications. Provided unconventional consulting and augmented Hadoop eco-system with custom engineering work. Participant in the Consortium of Data Governance Initiative. Worked in a highly critical and pressurized environment with an exposure to the Global CEO. * Consultant Architect at mid-sized financial client. Facilitated installation and configuration of Hortonworks Data Platform (HDP). Led ETL from clients’ data sources, developed Hive schema, wrote Pig scripts, and created HBase Java clients, generated reports and presented in Excel and Jasper.* Working directly with clients’ management and technical personnel to deliver Big Data solutions beyond Statement of Work (SoW). * Helping designing and implementing Hadoop architectures and configurations for customers, providing both advisory and resource augmentation.* Driving projects with clients to successful completion. This includes wearing multiple hats from System Admin to Application Developer.* Working closely with Hortonworks’ teams at all levels to help ensure the success of consulting engagements.* Analyzing complex distributed production deployments, and make recommendations to optimize performance. * Documenting and presenting complex architectures for the clients’ technical teams.* Keeping current with the Hadoop ecosystem. Attending speaking engagements time permitting.* Up to 75% travelling.Major Development Tools: HDInsight, Azure, HDP 2.x, Hadoop, HDFS, Hive, Pig, HBase, Sqoop, Kerberos, MySQL, Java, Metro-II, Microsoft Excel, Intellij, Eclipse, Github, Linux, Windows.IBM/YORK UNIVERSITY, Toronto, Canada (www.ibm.com/ca/en/)	Dec 13 – July 14 (8m)IBM is a multi-national technology and consulting corporation with expertise in data management and analytics. Big Data Scientist and Architect (contract)* Worked on a highway traffic monitoring and analysis project (CVST) for Greater Toronto Area (GTA), and dealing with large amounts of traffic data. * Proposed a data platform, Godzilla, to ingest real-time data (using HBase over HDFS) coming from multiple sources including sensors, video cameras, mobile phones and twitter.* Aimed to mine data in Godzilla (using Mahout) to identify anomalies, clusters, build prediction models and recommender systems.* Conducted a PoC by first building a MySQL data warehouse (data service) using a star schema with three months of traffic sensor data (300GB). Provided a web-interface to the data service using phpmyadmin.* Analyzed data with Weka for outlier detection, cluster identification and correlation evaluation.* Extracted key-performance-indicators (KPIs) and visualized weekly patterns that are of interest to Ministry of Transport (MTO).* Set up multiple Cloudera clusters (CDH4) over OpenStack compliant cloud (SAVI). Explored MapReduce job bursting between Cloudera clusters. Provided an intelligent controller to manage job bursting based on the metrics extracted from Hadoop master (JobTracker). Developed cost and performance models to facilitate the controller agent. * Installed and managed Cloudera Hadoop distribution (CDH5) over SAVI. Sqoop-ed data from the data service to HBase/HDFS cluster. * Aggregated data using HBase APIs. Further data processing and ad-hoc analytics by mapping Pig and Hive over HBase data.* Co-supervision of two MSc students.Major Development Tools: 	Cloudera (CDH4 & CDH5), Hadoop, HDFS, HBase, Pig, Hive, RapidMiner/Weka, Sqoop, Solr, MySQL, CloverETL, OpenStack, Java, Microsoft Excel, Eclipse, CVS, Linux, Windows.QUEEN’S UNIVERSITY, Kingston, Canada (www.queensu.ca)		Aug 09 – Nov 13 (4y 4m)One of Canada’s leading universities. PhD in Cloud Computing, IT Consultant, System Admin, Career Consultant* Research area: Lowering Deployment Cost for Data-Intensive Applications in Public Clouds* Consulting work to Gnowit in the use of cloud computing to scale out Online Media Monitoring platform from Canada to USA.* Explored Workflow Management Systems (WfMSs) that execute workflows of short runtime services with REST-ful and/or feather-weight interfaces. Investigated template VMs for modules with both types of interfaces in the Amazon cloud.* Explored elasticity strategies for Gnowit such that the number of used VMs can increase and decrease dynamically with the current need (e.g. measured in VM utilization).* Erected benchmarking infrastructure for web-applications supported by NoSQL platforms including HBase, Cassandra and PNUTS. Executed NoSQL workloads, and used Flume to transport web-server logs into HDFS for further analysis and processing. Also analyzed benchmark results using RapidMiner/Weka data mining toolkits.* Installed Cloudera Hadoop distribution (CDH) in a local cluster that included HDFS, Hive, Pig and Mahout.* Used Sqoop to transfer data from TPC database benchmarks into CDH.* Compared the scalability of executing analytical database in MySQL against CDH, and analyzed the results using IBM Many Eyes of IBM Cognos.* Experimented with machine learning over Hadoop with Radoop and Mahout.* Explored dollar cost of executing data-intensive workloads in local CDH versus Amazon’s Elastic MapReduce (EMR).* Developed performance and cost models to quantify the dollar-cost of deploying data-intensive applications in Amazon EC2 clouds. * Developed a data service using MySQL in the Amazon cloud.1* Developed a prediction model to forecast the behaviour of a workload execution in a multi-partition database system in the Amazon cloud. Used R and Weka in building the model.  * Developed a cost model that quantifies the dollar-cost of deploying data-intensive applications in the Amazon cloud. The model accounts for any workload type (analytical, transactional or mixed) and models costs for all the resources used in the workload execution.* Developed algorithms that search for the minimal dollar-cost deployment.* Collaborative research with Spanish and Queen’s academics, and built teams from siloed researchers.* Systematic study of Big Data processing platforms, parallel database systems (Vertica, Teradata, GreenPlum and Aster data) and provisioning techniques in public clouds. * Part-time system administrator at Queen’s University International Center (QUIC).* Extended web-databases for record special needs of a minority, exposure to VP.* Researched market to recommend computer systems, saving thousands of dollars.* Upgraded computer systems and re-purposed old systems for library use.* Introduced Wiki for tracking documentation (replacing orthodox documentation process using word).* Introduced source control (CVS) and proposed issue tracking tools such as bugzilla instead of tracking using emails.* Liaised with non-technical staff for project requirements, explaining and managing their expectations.* Reduced human effort in managing inventory databases by distilling attributes.* Transitioned out smoothly by training replacement.* Volunteered assistance to graduate students in their projects and assignments.* Career consultant at Queen’s University for both undergraduate and graduate students.Major Development Tools: 	Hadoop, HDFS, Cassandra, Hive, IBM Many Eyes, RapidMiner/Weka, Amazon EC2, S3, EBS, DynamoDB, Amazon Cloud Market, Java, Microsoft Excel, Eclipse, CVS, Linux, Windows. CITIGROUP, Toronto, Canada (www.citi.com)		Sep 08-Apr 09 (8m)A multinational financial services giant.Grid Development Engineer (contract)* Worked in a global grid computing team which aimed at providing a centralized grid across Citigroup.* Identified and evaluated advanced/emerging technologies in grid and cloud computing.* Erected a centralized compute grid (Symphony) as part of a team. The compute grid efficiently harnessed the computation of commodity servers but bottlenecked on data access for data-intensive applications. Evaluated the suitability of compute grid augmented with centralized data-caches (Gemfire, kdb) to provide efficient and scalable data access. Grid monitoring using Ganglia.* Developed a comprehensive test plan for data-intensive applications Symphony.* Mentoring and training the use of grids and testing framework.* Improved software development process by setting up source control system (CVS).Major Development Tools: Symphony, ETL, Ganglia, Java/C++, Gemfire, kdb, Microsoft Excel, CVS, Eclipse, Linux, Windows.ACCENTURE, Toronto, Canada (www.accenture.com)		 Jun 08 – Aug 08 (3m)Global management consulting, technology services and outsourcing corporation.Analyst and Senior Software Developer (contract)* Developed high performance and high availability feed processing infrastructure to process real-time data from electronic markets for Morgan Stanley as part of a global team. * In particular, developed feed infrastructure (29k loc), using highly generic infrastructural and order book libraries (3k loc), to manage sub-milliseconds ITCH messages from Nasdaq OMX Europe. Nasdaq OMX Europe is alive and open for trading.* OMX feed infrastructure is designed to be scalable to process large amount of messages per sec averaging at 15,000 to 20,000. Message rate can change drastically during the trading day. Automatic trading algorithms operate on the data processed. Therefore, financial stake is associated with timely processing. Consequently, latency requirements are stringent.Major Development Tools: Advanced C++, Traits, Templates, CVS, Linux.PLATFORM COMPUTING (now IBM), Toronto, Canada (www.platform.com)	Sep 06 – Mar 08 (1y 8m)Develops distributed computing and computational grid middleware.Software Developer* Worked on 6 projects in Symphony team. Symphony is a service-oriented architecture (SOA) consisting of middleware (soam with 500k loc); and a grid resource allocation manager (ego with 700k loc).* Implemented Service Replay Debugger that enables customers to reproduce the errors occurred in a distributed application. It also facilitates support team in rectifying customer issues. * Job Resource Broker or JRB is a meta-scheduler that directs clients of Symphony to the scheduler of a specific Symphony cluster. JRB aims to address the problem of load balancing across clusters. Researched into making JRB a part of the general product. * Resource Manager (resmgr) is a component that binds soam and ego. Led resmgr refactoring project by managing inter-team communications and politics. Delivered refactored design of resmgr in the presence of limited documentation by abstracting current behaviour.* Symphony's excel adapter enables excel applications to run on a Symphony cluster. Compared excel adapter of Symphony with other excel on grids such as Excel Grid.* Revised and documented Frequently Asked Questions (FAQs) for Symphony.* Worked on 1 project in Load Sharing Facility (LSF). LSF (1.8m loc) is company’s research-turned-commercial flagship product. LSF is a computational batch job scheduling system.* LSF is a 14 years old mature product written primarily in C. Any development or defect resolution required reverse engineering and ensuring the solution was backward compatible across all 10 platforms and 6 compilers.* Used Scrum agile development model for reducing uncertainty and targeting predictable releases.* Acting chair of team meetings.* Implemented “Hot fixes” for customers like JPMorganChase.* Major Development Tools:	C/C++, .NET, Java, C#, Microsoft Visual Studio 2003, Eclipse, Purify, CVS, Linux, Windows.PICDAR, London, UK (www.picdar.com)		   Nov 05 – Apr 06 (6m)Develops content management and automated media workflow applications.Software Engineer (Internship)* Primarily worked on content management for digital media. In particular, developed web-applications using Java/J2EE. This included dealing with all three tiers of web-applications and tools including Java Page Flows/Struts, JSP, Java Servlets, JDBC, Tag Libraries, JavaScript and Ant scripts. The persistence layer involved dealing with multiple databases, in particular Oracle and PostgreSQL. * Led performance testing of PostgreSQL and Oracle. Devised a test strategy and experiment method. Developed scripts to generate 'test data' and populate PostgreSQL and Oracle. Led migration of PostgreSQL to Oracle.* Needed to compare the retrieval times of search indexes for various databases with web-applications. Devised an evaluation strategy and an experiment method. Evaluation metric was latency of fetching search results for a given keyword. To avoid biasness of experiments, investigated different strategies for populating the databases. Using one of the strategies, databases were populated with words chosen at random from the dictionary. Then, using a keyword at random, fetched the results from the databases and measured latency. Repeated the experiments and applied statistical techniques to ensure the sanity of the results. In hindsight, this process also allowed analysis of quality of search indexes.* Investigated performance bottlenecks in software development process. In particular, identified two major causes: (i) exclusive locking of parts of source tree in CVS during development, and (ii) manual regression testing. Rectified (i) by suggesting improved usage of CVS, and (ii) by developing JUnit tests.Major Development Tools:	Java/J2EE, Web-application development Tools, JDBC, JUnit, CVS, Jboss, Mac OS.TRANSITIVE (now IBM), Manchester, UK (www.transitive.com)		     Aug 02 – Mar 04 (1y 8m)Develops software to allow dynamic translation of a binary across architectures.Software Engineer* Developed dynamic binary translator (QuickTransit) using effective compiler technologies in C/C++. QuickTransit performs runtime decoding of executables into intermediate representation (IR), performs optimizations on IR, and then translates IR to allow execution on target hardware. QuickTransit is now part of Intel-based Macs (Rosetta2) that allows old Mac applications to run on new Intel-based Macs.* QuickTransit translation is aimed to be bit-wise accurate. The nature of the product requires rigorous testing. Consequently, the testing is automated and consists of independently executable jobs. Such jobs are beyond the capacity of a single computing resource. Harnessed the computing power of a compute farm for execution of such jobs. This was achieved by developing a Perl test harness as part of a team. The test harness uses network queuing system (specifically GNQS) to schedule and manage testing jobs in a computational cluster. The test harness is used in production for development, code reviews, regression testing and releases of DBT.* Explored visualization techniques to facilitate understanding of the virtual memory of QuickTransit. QuickTransit generated and modified the target binary, and the virtual memory of DBT was bulky and complex. Modelled the virtual memory as a 3D world that presented the bigger picture with an ability to “zoom-in” in a particular area of memory.* Developed CodeCoverage quality assurance management tool in Perl used by both management and engineers. * Developed Coding Standard Inspection (CSI) tool to standardize the code development.* Automated product testing by developing a distributed Perl test harness to significantly improve engineers' productivity.* Managed engineers’ development worlds and released code baselines on a rotational basis.* Responsible for overseeing all computer memory related testing for QuickTransit.* Actively involved in recruiting campaign. Presented before a large audience (170 students). Major Development Tools:	C++, Perl, OpenGL, GDB,  XML, Mpatrol, Vmalloc, CVS, Source Navigator, Linux.CISCO SYSTEMS, Reading, UK (www.cisco.com)		Aug 01 - Jul 02 (1y)Designs, manufactures, and sells networking equipment..Software Engineer (Internship)* Worked as a key team player on a complex telecommunication and VoIP product i.e. H.323 Signalling Interface.* Liaised with Cisco partner in Germany for software development. * Used C++ and Tcl/tk to develop Call Forwarding and Redirection features on the core product.* Multi-tasking by pursuing day to day activities, cross-team assignments and Cisco certification (CCNA).Major Development Tools:	C++, Tcl/tk, GDB, CVS, ClearCase, Solaris.Masters (by research) in Computer Science (Grid Computing)The University of Manchester, UK (www.cs.manchester.ac.uk), Jul 04 – Aug 05BSc in Computer Science with Industrial Experience The University of Manchester, UK (www.cs.manchester.ac.uk), Sep 99 – Jun 031 The image (ami-7bc16e12) is publicly available at: http://thecloudmarket.com/owner/966178113014. Once the image is instantiated, the clients can connect (ssh in) to the instance and access the MySQL dbms as root user with wlmgmt password.2 http://thenextweb.com/insider/2011/10/22/how-one-of-apples-most-important-pieces-of-software-came-from-a-small-uk-startup/ --------------------------------------------------------------------------------------------------------------2-6-			13-Jan-18 ©	